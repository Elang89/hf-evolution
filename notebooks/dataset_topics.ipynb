{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb7aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer, WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import ne_chunk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sqlalchemy import create_engine\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0fe81ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/elang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/elang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/elang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/elang/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "conn = create_engine('postgresql://root:password@localhost:5432/hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33de688",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"\"\"SELECT DISTINCT ON (commit_hash) commit_message\n",
    "    FROM hf_commits\"\"\", con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b076f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commit_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>initial commit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        commit_message\n",
       "count             7522\n",
       "unique            2662\n",
       "top     initial commit\n",
       "freq               912"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972295e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(message):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(message)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in word_tokens \n",
    "              if token not in stop_words and len(token) > 3]\n",
    "    cleaned_text = \" \".join(tokens)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e61a2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"commit_message\"] = df[\"commit_message\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f6acc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commit_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>initial commit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>initial commit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>initial commit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Upload CryptoPunks.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Update squad_multitask.py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              commit_message\n",
       "0             initial commit\n",
       "1             initial commit\n",
       "2             initial commit\n",
       "3      Upload CryptoPunks.py\n",
       "4  Update squad_multitask.py"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753f06b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7522, 1000)\n",
      "  (0, 208)\t0.6966280483300364\n",
      "  (0, 482)\t0.7174324792479668\n",
      "  (1, 208)\t0.6966280483300364\n",
      "  (1, 482)\t0.7174324792479668\n",
      "  (2, 208)\t0.6966280483300364\n",
      "  (2, 482)\t0.7174324792479668\n",
      "  (3, 690)\t0.4794475725490822\n",
      "  (3, 251)\t0.833599061963867\n",
      "  (3, 913)\t0.2743038991151113\n",
      "  (4, 803)\t0.88800801917058\n",
      "  (4, 910)\t0.20744436551815662\n",
      "  (4, 690)\t0.4103761604960895\n",
      "  (5, 208)\t0.6966280483300364\n",
      "  (5, 482)\t0.7174324792479668\n",
      "  (6, 910)\t1.0\n",
      "  (7, 558)\t0.5228597249372511\n",
      "  (7, 711)\t0.5056324788577862\n",
      "  (7, 243)\t0.686260522223647\n",
      "  (8, 385)\t0.9444650662151614\n",
      "  (8, 258)\t0.3286118359085547\n",
      "  (9, 841)\t0.26416331029925355\n",
      "  (9, 721)\t0.26463492384369514\n",
      "  (9, 461)\t0.261182836967107\n",
      "  (9, 203)\t0.26072698940692574\n",
      "  (9, 426)\t0.2613134568034284\n",
      "  :\t:\n",
      "  (7517, 913)\t0.30975310137371664\n",
      "  (7518, 938)\t0.5651920966008608\n",
      "  (7518, 526)\t0.26400321692479306\n",
      "  (7518, 424)\t0.26400321692479306\n",
      "  (7518, 640)\t0.27282130119818565\n",
      "  (7518, 3)\t0.4177815439363102\n",
      "  (7518, 2)\t0.4171032900734515\n",
      "  (7518, 258)\t0.2566996089218622\n",
      "  (7518, 913)\t0.22873515732093183\n",
      "  (7519, 135)\t0.8471253461104501\n",
      "  (7519, 910)\t0.23972989222065222\n",
      "  (7519, 690)\t0.4742449016627638\n",
      "  (7520, 460)\t1.0\n",
      "  (7521, 841)\t0.26416331029925355\n",
      "  (7521, 721)\t0.26463492384369514\n",
      "  (7521, 461)\t0.261182836967107\n",
      "  (7521, 203)\t0.26072698940692574\n",
      "  (7521, 426)\t0.2613134568034284\n",
      "  (7521, 458)\t0.2604674231687233\n",
      "  (7521, 614)\t0.26463492384369514\n",
      "  (7521, 719)\t0.2642305493784109\n",
      "  (7521, 530)\t0.26463492384369514\n",
      "  (7521, 285)\t0.5226269136068568\n",
      "  (7521, 387)\t0.24741222607644006\n",
      "  (7521, 910)\t0.20887865976499914\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(stop_words=stop_words, max_features=1000)\n",
    "vect_text = vect.fit_transform(df[\"commit_message\"])\n",
    "\n",
    "print(vect_text.shape)\n",
    "print(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ae1eca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update zxnrm9\n",
      "2.0620690081115702\n",
      "9.232573093000276\n"
     ]
    }
   ],
   "source": [
    "idf = vect.idf_\n",
    "dd = dict(zip(vect.get_feature_names_out(), idf))\n",
    "l = sorted(dd, key=(dd).get)\n",
    "\n",
    "print(l[0], l[-1])\n",
    "print(dd[\"update\"])\n",
    "print(dd[\"zxnrm9\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fa33f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n",
    "lsa_top=lsa_model.fit_transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d754354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.65999768e-02  9.99570406e-01 -3.16992185e-03 ...  5.72348639e-03\n",
      "  -3.09401679e-06 -4.00539277e-03]\n",
      " [ 1.65999768e-02  9.99570406e-01 -3.16992185e-03 ...  5.72348639e-03\n",
      "  -3.09401679e-06 -4.00539277e-03]\n",
      " [ 1.65999768e-02  9.99570406e-01 -3.16992185e-03 ...  5.72348639e-03\n",
      "  -3.09401679e-06 -4.00539277e-03]\n",
      " ...\n",
      " [ 6.41197041e-02 -9.43506732e-04  3.53236600e-03 ... -2.30714138e-02\n",
      "   6.77256083e-07  1.99293252e-01]\n",
      " [-3.48994792e-17 -7.64697186e-18  4.30582687e-17 ... -1.33157563e-07\n",
      "   9.99999903e-01  9.37849111e-06]\n",
      " [ 9.93649491e-01 -1.69369505e-02 -1.01194131e-02 ...  7.34357119e-03\n",
      "  -1.33778850e-06 -2.22471782e-03]]\n",
      "(7522, 10)\n"
     ]
    }
   ],
   "source": [
    "print(lsa_top)\n",
    "print(lsa_top.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94ac3c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 :\n",
      "Topic  0  :  1.6599976844997735\n",
      "Topic  1  :  99.95704055306686\n",
      "Topic  2  :  -0.3169921848347223\n",
      "Topic  3  :  0.0051059943322054775\n",
      "Topic  4  :  0.006928706006512238\n",
      "Topic  5  :  -0.007288855245144336\n",
      "Topic  6  :  -1.083122968753771\n",
      "Topic  7  :  0.5723486389639196\n",
      "Topic  8  :  -0.0003094016787536982\n",
      "Topic  9  :  -0.4005392769700201\n"
     ]
    }
   ],
   "source": [
    "l=lsa_top[0]\n",
    "print(\"Document 0 :\")\n",
    "for i,topic in enumerate(l):\n",
    "  print(\"Topic \",i,\" : \",topic*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e84a592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1000)\n",
      "[[ 4.86740195e-07  3.65086310e-06  5.86481307e-04 ...  1.09711500e-04\n",
      "   2.27045181e-07  3.84114522e-06]\n",
      " [ 3.32531818e-07  5.90573313e-08  2.57403341e-04 ...  6.90701760e-06\n",
      "   7.42587847e-08 -9.14475718e-08]\n",
      " [ 1.56098759e-04  1.29795810e-05  1.22173704e-01 ...  5.00570880e-05\n",
      "   6.77139943e-06 -1.22822527e-07]\n",
      " ...\n",
      " [ 1.40166586e-04 -1.21074650e-05  4.65889537e-01 ...  1.68854097e-04\n",
      "   1.54868083e-04  4.43553723e-06]\n",
      " [-2.57339609e-07  5.79153254e-08 -1.08927059e-05 ...  8.69754882e-07\n",
      "  -7.40848667e-07 -2.67475048e-09]\n",
      " [ 2.61298879e-04  1.13223470e-04 -1.93439720e-01 ...  1.85679147e-03\n",
      "   2.81403923e-04 -2.21777928e-06]]\n"
     ]
    }
   ],
   "source": [
    "print(lsa_model.components_.shape) # (no_of_topics*no_of_words)\n",
    "print(lsa_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07f87a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "datasets file library note releases release tag github huggingface com \n",
      "\n",
      "Topic 1: \n",
      "commit initial file transformers data init 218e496519ff14b4bc69ea559616af6f2ef89e57 first parquet upload \n",
      "\n",
      "Topic 2: \n",
      "data upload git lfs parquet train 00494 00000 00001 test \n",
      "\n",
      "Topic 3: \n",
      "readme md update create py json qalb test test_ldkp delete \n",
      "\n",
      "Topic 4: \n",
      "json dataset_infos upload test update qalb csv py delete jsonl \n",
      "\n",
      "Topic 5: \n",
      "update py test_ldkp qalb data parquet test dataset metrics 00494 \n",
      "\n",
      "Topic 6: \n",
      "data file 00000 00001 test json qalb commit validation added \n",
      "\n",
      "Topic 7: \n",
      "00000 00001 test csv validation upload git lfs delete jsonl \n",
      "\n",
      "Topic 8: \n",
      "huggingartists file 01233 train json git lfs parquet qalb dataset \n",
      "\n",
      "Topic 9: \n",
      "csv py create delete file upload test test_ldkp data jsonl \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elang/Projects/Python/hf-evolution/.venv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vocab = vect.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(lsa_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bb4797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model=LatentDirichletAllocation(n_components=10,learning_method='online',random_state=42,max_iter=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba23e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_top=lda_model.fit_transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28807bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7522, 10)\n",
      "[[0.04142398 0.04142398 0.04142398 ... 0.04142398 0.04142398 0.04142398]\n",
      " [0.04142398 0.04142398 0.04142398 ... 0.04142398 0.04142398 0.04142398]\n",
      " [0.04142398 0.04142398 0.04142398 ... 0.04142398 0.04142398 0.04142398]\n",
      " ...\n",
      " [0.0390533  0.03904573 0.64857814 ... 0.03904596 0.03904612 0.0390473 ]\n",
      " [0.05       0.05       0.05       ... 0.05       0.05       0.05      ]\n",
      " [0.02301723 0.02301546 0.02301623 ... 0.02301561 0.02301552 0.79285798]]\n"
     ]
    }
   ],
   "source": [
    "print(lda_top.shape)\n",
    "print(lda_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a52685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "sum=0\n",
    "for i in lda_top[0]:\n",
    "  sum=sum+i\n",
    "print(sum)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb168ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0: \n",
      "Topic  0 :  4.142398220802241 %\n",
      "Topic  1 :  4.142398211989103 %\n",
      "Topic  2 :  4.142398206803978 %\n",
      "Topic  3 :  4.142398206682384 %\n",
      "Topic  4 :  4.142398219405575 %\n",
      "Topic  5 :  4.142407526460775 %\n",
      "Topic  6 :  62.7184067418854 %\n",
      "Topic  7 :  4.1423982068799265 %\n",
      "Topic  8 :  4.142398206781031 %\n",
      "Topic  9 :  4.142398252309597 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Document 0: \")\n",
    "for i,topic in enumerate(lda_top[0]):\n",
    "  print(\"Topic \",i,\": \",topic*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58b7f2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9937703  0.10387214 0.10383334 ... 0.10446433 0.1035593  0.10403754]\n",
      " [0.10442454 0.10415555 0.1356207  ... 0.10392133 0.10412554 0.10514369]\n",
      " [0.10321015 0.10391832 0.10422119 ... 2.47841244 0.10368016 0.10381942]\n",
      " ...\n",
      " [0.1035873  0.10613124 0.1040642  ... 0.10396642 0.10315627 0.1247359 ]\n",
      " [0.10455492 0.11129556 0.1038151  ... 0.10424846 2.28564331 0.10384095]\n",
      " [0.10352771 0.10358448 0.10452014 ... 0.10415669 0.10323042 0.10341478]]\n",
      "(10, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.components_)\n",
    "print(lda_model.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2ad8b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "readme md update create jsonl adding first gz sentihood folder \n",
      "\n",
      "Topic 1: \n",
      "txt added loader 00011 00003 debugging upload valid 00005 fixed \n",
      "\n",
      "Topic 2: \n",
      "py 18 update rename metrics attempt image alffa_amharic batch13 scifi_tv_shows \n",
      "\n",
      "Topic 3: \n",
      "data 00000 parquet 00001 lfs git upload train metadata validation \n",
      "\n",
      "Topic 4: \n",
      "upload train git lfs parquet data 00494 json huggingartists dataset_infos \n",
      "\n",
      "Topic 5: \n",
      "dataset script card name init updated loading dummy info change \n",
      "\n",
      "Topic 6: \n",
      "commit initial test_ldkp version code transformers question recon bumbp datset \n",
      "\n",
      "Topic 7: \n",
      "sample flagged split path link download track file changed config \n",
      "\n",
      "Topic 8: \n",
      "csv delete upload test gitattributes import raw feature gec val \n",
      "\n",
      "Topic 9: \n",
      "datasets file tag huggingface github release http com note library \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, comp in enumerate(lda_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7503c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
